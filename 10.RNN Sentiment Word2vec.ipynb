{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Date    : 2017-09-10\n",
    "# @Author  : Ivan\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from wordvec import WordEmbedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, SimpleRNN, GlobalAveragePooling1D\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-s SIZE] [-l RATE] [-e EPOCH] [-m MODEL]\n",
      "                             [-n NAME] [-d SEED]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\cxin\\AppData\\Roaming\\jupyter\\runtime\\kernel-d7160897-7574-45c2-bc3f-8f047b3d5903.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cxin\\AppData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-s', '--size', help='The size of each batch used to be trained.', type=int, default=100)\n",
    "parser.add_argument('-l', '--rate', help='Learning rate of AdaGrad.', type=float, default=0.02)\n",
    "parser.add_argument('-e', '--epoch', help='Number of the training epoch.', type=int, default=25)\n",
    "parser.add_argument('-m', '--model', help=\"Which model to use\", type=str, default=\"rnn\")\n",
    "parser.add_argument('-n', '--name', help='Name used to save the model.', type=str, default=\"sentiment\")\n",
    "parser.add_argument('-d', '--seed', help='Random seed used for generation.', type=int, default=42)\n",
    "\n",
    "# Compile and configure all the parameters.\n",
    "args = parser.parse_args()\n",
    "np.random.seed(args.seed)\n",
    "tf.set_random_seed(args.seed)\n",
    "# Logging configuration.\n",
    "# Set the basic configuration of the logging system.\n",
    "log_formatter = logging.Formatter(fmt='%(asctime)s [%(processName)s, %(process)s] [%(levelname)-5.5s]  %(message)s', \n",
    "                                  datefmt='%m-%d %H:%M')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# File logger\n",
    "file_handler = logging.FileHandler(\"{}.log\".format(args.name))\n",
    "file_handler.setFormatter(log_formatter)\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(file_handler)\n",
    "# Stderr logger\n",
    "std_handler = logging.StreamHandler(sys.stdout)\n",
    "std_handler.setFormatter(log_formatter)\n",
    "std_handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(std_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c59291781e1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmr_positive_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Positive Instances: %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mmr_txt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mmr_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "# This script is used to train and test on the Movie-Review dataset\n",
    "# Since this data set is not explicitly partitioned into training/test \n",
    "# split, we use 0.7/0.3 partition to as the train/test split.\n",
    "mr_positive_filename = 'rt-polarity.pos'\n",
    "mr_negative_filename = 'rt-polarity.neg'\n",
    "# Loading and building training and test data set.\n",
    "mr_txt, mr_label = [], []\n",
    "start_time = time.time()\n",
    "# Read all the instances \n",
    "with open(mr_positive_filename, 'r') as fin:\n",
    "    lines = fin.readlines()\n",
    "    logger.info('Positive Instances: %d' % len(lines))\n",
    "    mr_txt.extend(lines)\n",
    "    mr_label.extend([1] * len(lines))\n",
    "with open(mr_negative_filename, 'r') as fin:\n",
    "    lines = fin.readlines()\n",
    "    logger.info('Negative Instances: %d' % len(lines))\n",
    "    mr_txt.extend(lines)\n",
    "    mr_label.extend([0] * len(lines))\n",
    "# Shuffling all the data.\n",
    "assert len(mr_txt) == len(mr_label)\n",
    "data_size = len(mr_txt)\n",
    "logger.info('Size of the data sets: %d' % data_size)\n",
    "random_index = np.arange(data_size)\n",
    "np.random.shuffle(random_index)\n",
    "mr_txt = list(np.asarray(mr_txt)[random_index])\n",
    "mr_label = list(np.asarray(mr_label)[random_index])\n",
    "end_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# This script is used to train and test on the Movie-Review dataset\n",
    "# Since this data set is not explicitly partitioned into training/test \n",
    "# split, we use 0.7/0.3 partition to as the train/test split.\n",
    "mr_positive_filename = './mr-polarity.pos'\n",
    "mr_negative_filename = './mr-polarity.neg'\n",
    "# Loading and building training and test data set.\n",
    "mr_txt, mr_label = [], []\n",
    "start_time = time.time()\n",
    "# Read all the instances \n",
    "with file(mr_positive_filename, 'r') as fin:\n",
    "    lines = fin.readlines()\n",
    "    logger.info('Positive Instances: %d' % len(lines))\n",
    "    mr_txt.extend(lines)\n",
    "    mr_label.extend([1] * len(lines))\n",
    "with file(mr_negative_filename, 'r') as fin:\n",
    "    lines = fin.readlines()\n",
    "    logger.info('Negative Instances: %d' % len(lines))\n",
    "    mr_txt.extend(lines)\n",
    "    mr_label.extend([0] * len(lines))\n",
    "# Shuffling all the data.\n",
    "assert len(mr_txt) == len(mr_label)\n",
    "data_size = len(mr_txt)\n",
    "logger.info('Size of the data sets: %d' % data_size)\n",
    "random_index = np.arange(data_size)\n",
    "np.random.shuffle(random_index)\n",
    "mr_txt = list(np.asarray(mr_txt)[random_index])\n",
    "mr_label = list(np.asarray(mr_label)[random_index])\n",
    "end_time = time.time()\n",
    "# Record timing\n",
    "logger.info('Time used to load and shuffle MR dataset: %f seconds.' % (end_time-start_time))\n",
    "# Load word-embedding\n",
    "embedding_filename = './wiki_embeddings.txt'\n",
    "# Load training/test data sets and wiki-embeddings.\n",
    "word_embedding = WordEmbedding(embedding_filename)\n",
    "embed_dim = word_embedding.embedding_dim()\n",
    "start_time = time.time()\n",
    "blank_index = word_embedding.word2index('</s>')\n",
    "logger.info('Blank index: {}'.format(word_embedding.index2word(blank_index)))\n",
    "# Word-vector representation, zero-padding all the sentences to the maximum length.\n",
    "max_len = 52\n",
    "mr_insts = np.zeros((data_size, max_len, word_embedding.embedding_dim()), dtype=np.float32)\n",
    "mr_label = np.asarray(mr_label)[:, np.newaxis]\n",
    "for i, sent in enumerate(mr_txt):\n",
    "    words = sent.split()\n",
    "    words = [word.lower() for word in words]\n",
    "    l = len(words)\n",
    "    # vectors = np.zeros((len(words)+2, embed_dim), dtype=np.float32)\n",
    "    mr_insts[i, 1: l+1, :] = np.asarray([word_embedding.wordvec(word) for word in words])\n",
    "    mr_insts[i, 0, :] = mr_insts[i, l+1, :] = word_embedding.wordvec(\"</s>\")\n",
    "end_time = time.time()\n",
    "logger.info('Time used to build sparse and dense input word-embedding matrices: %f seconds.' % (end_time-start_time))\n",
    "logger.info(\"Shape of data tensor = {}, shape of label matrix = {}\".format(mr_insts.shape, mr_label.shape))\n",
    "# Compute the balance of positive/negative count.\n",
    "p_count = np.sum(mr_label)\n",
    "logger.info('Default positive percentage in dataset: %f' % (float(p_count) / data_size))\n",
    "logger.info('Default negative percentage in dataset: %f' % (float(data_size-p_count) / data_size))\n",
    "# Use 0.7/0.3 partition of the whole data.\n",
    "num_classes = 2\n",
    "num_train = int(data_size * 0.7)\n",
    "num_test = data_size - num_train\n",
    "logger.info(\"Training set size = {}, test set size = {}\".format(num_train, num_test))\n",
    "train_insts, train_labels = mr_insts[:num_train, :, :], to_categorical(mr_label[:num_train, :], num_classes)\n",
    "test_insts, test_labels = mr_insts[num_train:, :, :], to_categorical(mr_label[num_train:, :], num_classes)\n",
    "# Initialize model training configuration\n",
    "learn_rate = args.rate\n",
    "batch_size = args.size\n",
    "epoch = args.epoch\n",
    "# Number of hidden units in RNN.\n",
    "num_hidden = 100\n",
    "# Different model to use, training phase.\n",
    "# Your code here: use the following handler model to name your Keras model.\n",
    "model = None\n",
    "if args.model == \"rnn\":\n",
    "    # You should implement your vanilla RNN + mean pooling here.\n",
    "    model = Sequential()\n",
    "    # input_shape = (None, num_feats), where the first None means the length of the sequence and the \n",
    "    # second num_feats means the number of features. \n",
    "    model.add(SimpleRNN(num_hidden, input_shape=(None, word_embedding.embedding_dim()), return_sequences=True))\n",
    "    # Mean pooling along time dimension.\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    # Logistic regression classification.\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adadelta\", metrics=[\"accuracy\"])\n",
    "elif args.model == \"gru\":\n",
    "    # You should implement your GRU + mean pooling here.\n",
    "    model = Sequential()\n",
    "    # input_shape = (None, num_feats), where the first None means the length of the sequence and the \n",
    "    # second num_feats means the number of features. \n",
    "    model.add(GRU(num_hidden, input_shape=(None, word_embedding.embedding_dim()), return_sequences=True))\n",
    "    # Mean pooling along time dimension.\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    # Logistic regression classification.\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adadelta\", metrics=[\"accuracy\"])    \n",
    "elif args.model == \"lstm\":\n",
    "    # You should implement your LSTM + mean pooling here.\n",
    "    model = Sequential()\n",
    "    # input_shape = (None, num_feats), where the first None means the length of the sequence and the \n",
    "    # second num_feats means the number of features. \n",
    "    model.add(LSTM(num_hidden, input_shape=(None, word_embedding.embedding_dim()), return_sequences=True))\n",
    "    # Mean pooling along time dimension.\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    # Logistic regression classification.\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adadelta\", metrics=[\"accuracy\"])    \n",
    "else:\n",
    "    raise NameError(\"{} model not supported.\".format(args.model))\n",
    "\n",
    "# Training phase begins.\n",
    "logger.info(\"Finish building model: {}\".format(model.summary()))\n",
    "start_time = time.time()\n",
    "model.fit(train_insts, train_labels, epochs=epoch, batch_size=batch_size, verbose=2)\n",
    "end_time = time.time()\n",
    "logger.info(\"Time used for training the model = {} seconds.\".format(end_time - start_time))\n",
    "# Test phase.\n",
    "_, acc = model.evaluate(test_insts, test_labels, batch_size=batch_size, verbose=2)\n",
    "logger.info(\"Sentiment analysis accuracy with {} on test set = {}\".format(args.model, acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
